# ============================================
# OSD on GCP - Terraform Configuration Example
# ============================================
# Copy this file to terraform.tfvars and customize for your deployment
# cp terraform.tfvars.example terraform.tfvars
# ============================================

# ============================================
# REQUIRED: GCP Project & Cluster Settings
# ============================================

# GCP Project ID where resources will be created
gcp_project = "your-gcp-project-id"

# Name of the OSD cluster (used as prefix for all resources)
clustername = "my-osd-cluster"

# GCP Region for deployment
gcp_region = "us-central1"

# GCP Zone for single-zone deployments or bastion
gcp_zone = "us-central1-a"

# ============================================
# REQUIRED: Authentication Configuration
# ============================================

# Authentication type: "service_account" or "workload_identity_federation"
gcp_authentication_type = "workload_identity_federation"

# Path to GCP Service Account JSON file (required for service_account auth)
# gcp_sa_file_loc = "~/.ssh/osd-ccs-admin.json"

# ============================================
# OPTIONAL: Workload Identity Federation (WIF)
# ============================================

# Use existing WIF configuration instead of creating new
# Set to true to use an existing WIF config
use_existing_wif = false

# Name of existing WIF config (required when use_existing_wif = true)
# To list existing WIF configs: ocm gcp list wif-configs
# existing_wif_config_name = "my-existing-wif"

# ============================================
# VPC CONFIGURATION OPTIONS
# ============================================
# Choose ONE of the following options:
#   Option A: Create new VPC (default)
#   Option B: Use existing VPC
# ============================================

# --- Option A: Create New VPC (default) ---
use_existing_vpc = false

# VPC routing mode: "REGIONAL" or "GLOBAL"
vpc_routing_mode = "REGIONAL"

# CIDR blocks for new subnets (when creating new VPC)
master_cidr_block     = "10.0.0.0/19"
worker_cidr_block     = "10.0.32.0/19"
psc_subnet_cidr_block = "10.0.64.0/29"
bastion_cidr_block    = "10.10.0.0/24"

# --- Option B: Use Existing VPC ---
# Uncomment and configure the following to use pre-created VPC
# use_existing_vpc             = true
# existing_vpc_name            = "my-existing-vpc"
# existing_master_subnet_name  = "my-master-subnet"
# existing_worker_subnet_name  = "my-worker-subnet"
# existing_psc_subnet_name     = "my-psc-subnet"      # Required if osd_gcp_psc = true
# existing_router_name         = "my-router"          # Optional

# ============================================
# NAT Gateway Configuration
# ============================================

# Create NAT gateways for internet connectivity
# Set to false when using landing zone / hub-spoke architecture
enable_nat_gateway = true

# ============================================
# Proxy Configuration (for hub-spoke architecture)
# ============================================
# Required when enable_nat_gateway = false and using proxy for egress
# The proxy must be reachable from the OSD VPC (e.g., via VPC peering)

# HTTP proxy URL for cluster egress
# http_proxy = "http://10.100.0.10:3128"

# HTTPS proxy URL (usually same as http_proxy)
# https_proxy = "http://10.100.0.10:3128"

# Domains/CIDRs to bypass proxy (comma-separated)
# no_proxy = ".cluster.local,.svc,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.169.254"

# Path to CA bundle if proxy uses custom/self-signed certificate
# additional_trust_bundle = "/path/to/ca-bundle.pem"

# ============================================
# Cluster Type Configuration
# ============================================

# Deploy as private cluster (no public endpoints)
osd_gcp_private = false

# Enable Private Service Connect (PSC) for private Google API access
osd_gcp_psc = false

# PSC endpoint IP address (must be within Machine CIDR but outside all subnets)
# This IP is used for the Google APIs PSC endpoint
# Example: If your subnets are in 10.0.x.x range, use something like 10.0.255.100
# psc_endpoint_address = "10.0.255.100"

# List of GCP services for PSC endpoints (when osd_gcp_psc = true)
# enable_psc_endpoints = [
#   "storage.googleapis.com",
#   "container.googleapis.com",
#   "compute.googleapis.com",
#   "logging.googleapis.com",
#   "monitoring.googleapis.com"
# ]

# ============================================
# Multi-AZ Configuration
# ============================================

# Comma-separated availability zones for multi-AZ deployment
# Leave empty for single-zone deployment
# gcp_availability_zones = "us-central1-a,us-central1-b,us-central1-c"

# Number of compute/worker nodes
# For multi-AZ: must be multiple of zone count (e.g., 3, 6, 9 for 3 zones)
# For single-AZ: minimum 2 for CCS
# Leave as null for OCM defaults
# compute_nodes_count = 3

# Compute (worker) node machine type
# Run `ocm list machine-types --provider gcp` to see available options
# Examples:
#   - custom-4-32768-ext   (4 vCPU, 32GB RAM) - Default for OSD
#   - n2-standard-4        (4 vCPU, 16GB RAM)
#   - n2-standard-8        (8 vCPU, 32GB RAM)
#   - n2-highmem-4         (4 vCPU, 32GB RAM)
#   - e2-standard-4        (4 vCPU, 16GB RAM)
# Leave empty for OCM default
# compute_machine_type = "custom-4-32768-ext"

# ============================================
# Bastion Host Configuration
# ============================================

# Deploy bastion/jumphost for private cluster access
enable_osd_gcp_bastion = false

# Bastion VM machine type
bastion_machine_type = "e2-micro"

# Path to SSH public key for bastion access
bastion_key_loc = "~/.ssh/id_rsa.pub"

# ============================================
# Additional Machine Pools
# ============================================
# Configure additional machine pools beyond the default "worker" pool
# Pools are created AFTER the cluster is ready
# Run `ocm list machine-types --provider gcp` for available instance types

# Example: Single additional pool with fixed replicas
# additional_machine_pools = [
#   {
#     name          = "large"
#     instance_type = "n2-standard-16"
#     replicas      = 2
#     labels = {
#       "workload-type" = "large"
#     }
#   }
# ]

# Example: Multiple pools with autoscaling and taints
# additional_machine_pools = [
#   {
#     name          = "large"
#     instance_type = "n2-standard-16"
#     replicas      = 2
#     labels = {
#       "workload-type" = "large"
#     }
#   },
#   {
#     name          = "gpu"
#     instance_type = "n1-standard-4"
#     min_replicas  = 1
#     max_replicas  = 5
#     labels = {
#       "workload-type" = "gpu"
#     }
#     taints = [
#       {
#         key    = "nvidia.com/gpu"
#         value  = "true"
#         effect = "NoSchedule"
#       }
#     ]
#   },
#   {
#     name              = "zone-a-only"
#     instance_type     = "n2-standard-8"
#     replicas          = 3
#     availability_zone = "us-central1-a"  # Single zone in multi-AZ cluster
#   }
# ]

# Default: No additional machine pools
additional_machine_pools = []

# ============================================
# Advanced Options
# ============================================

# Deploy only infrastructure without OSD cluster
# Useful for testing VPC setup
only_deploy_infra_no_osd = false


# ============================================
# EXAMPLE CONFIGURATIONS
# ============================================

# --------------------------------------
# Example 1: Simple Public Cluster
# --------------------------------------
# gcp_project             = "my-project"
# clustername             = "my-cluster"
# gcp_region              = "us-central1"
# gcp_zone                = "us-central1-a"
# gcp_authentication_type = "workload_identity_federation"
# osd_gcp_private         = false
# osd_gcp_psc             = false

# --------------------------------------
# Example 2: Private Cluster with PSC
# --------------------------------------
# gcp_project             = "my-project"
# clustername             = "my-private-cluster"
# gcp_region              = "us-central1"
# gcp_zone                = "us-central1-a"
# gcp_authentication_type = "workload_identity_federation"
# osd_gcp_private         = true
# osd_gcp_psc             = true
# enable_osd_gcp_bastion  = true

# --------------------------------------
# Example 3: Multi-AZ Private Cluster with Existing VPC
# --------------------------------------
# gcp_project                  = "my-project"
# clustername                  = "my-multiaz-cluster"
# gcp_region                   = "us-central1"
# gcp_zone                     = "us-central1-a"
# gcp_authentication_type      = "workload_identity_federation"
# osd_gcp_private              = true
# osd_gcp_psc                  = true
# gcp_availability_zones       = "us-central1-a,us-central1-b,us-central1-c"
# compute_nodes_count          = 3
# use_existing_vpc             = true
# existing_vpc_name            = "my-vpc"
# existing_master_subnet_name  = "my-master-subnet"
# existing_worker_subnet_name  = "my-worker-subnet"
# existing_psc_subnet_name     = "my-psc-subnet"
# existing_router_name         = "my-router"
# enable_nat_gateway           = false
# enable_osd_gcp_bastion       = true

# --------------------------------------
# Example 4: Using Existing WIF
# --------------------------------------
# gcp_project              = "my-project"
# clustername              = "my-cluster"
# gcp_region               = "us-central1"
# gcp_zone                 = "us-central1-a"
# gcp_authentication_type  = "workload_identity_federation"
# use_existing_wif         = true
# existing_wif_config_name = "my-existing-wif"

# --------------------------------------
# Example 5: Hub-Spoke Architecture with Proxy
# (Fully private cluster using Squid proxy for egress)
# --------------------------------------
# Use setup-vpc-infrastructure.sh to create:
#   - Landing Zone VPC with NAT + Squid Proxy
#   - OSD VPC (private, no NAT)
#   - VPC Peering between them
#
# gcp_project                  = "my-project"
# clustername                  = "my-private-cluster"
# gcp_region                   = "us-central1"
# gcp_zone                     = "us-central1-a"
# gcp_authentication_type      = "workload_identity_federation"
# osd_gcp_private              = true
# osd_gcp_psc                  = true
# gcp_availability_zones       = "us-central1-a,us-central1-b,us-central1-c"
# compute_nodes_count          = 3
#
# # Use pre-created VPC from setup script
# use_existing_vpc             = true
# existing_vpc_name            = "my-cluster-vpc"
# existing_master_subnet_name  = "my-cluster-master-subnet"
# existing_worker_subnet_name  = "my-cluster-worker-subnet"
# existing_psc_subnet_name     = "my-cluster-psc-subnet"
# existing_router_name         = "my-cluster-router"
#
# # No NAT in OSD VPC - using proxy instead
# enable_nat_gateway           = false
#
# # Proxy in Landing Zone VPC (created by setup script)
# http_proxy                   = "http://10.100.0.10:3128"
# https_proxy                  = "http://10.100.0.10:3128"
# no_proxy                     = ".cluster.local,.svc,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,169.254.169.254"
#
# # Bastion will be in Landing Zone VPC
# enable_osd_gcp_bastion       = false
